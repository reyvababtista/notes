:PROPERTIES:
:ID:       df1f7289-1e78-4472-9006-d675e8d637d7
:ROAM_REFS: cite:yuCoCaContrastiveCaptioners2022
:END:
#+title: CoCa: Contrastive Captioners are Image-Text Foundation Models

* Addressed domains
Contrastive Captioner (CoCa) is an image-text foundation models that is
pretrained using a join contrastive and captioning losses. It addresses visual
and language modals with state-of-the-art performances on visual recognition,
multimodal understanding, and image captioning tasks.

* Architectural multimodality
CoCa
* Learning paradigm
CoCa is trained on image annotation data and noisy image-text data by treating
all labels as text.

* Key innovations
CoCa unifies single-encoder, dual-encoder, and encoder-decorder paradigms in
their training approach by leveraging image encoder, unimodal text decoder, and
multimodal text decoder that are trained with contrastive and captioning
(generative) losses. CoCa used a Transformer decoder and divided it into two
parts: unimodal and multimodal. The unimodal decoder only encode text
representations which then cascades into the multimodal decoder layer,
cross-attending to the image encoder outputs. This process enables CoCa to learn
the multimodal image-text representation. The contrastive learning was leveraged
by CoCa to learn global representations while captioning (generative) was used
for fine-grained region-level features.

* Limitations
