:PROPERTIES:
:ID:       7805bc33-51bf-47be-b404-439d4cdd2ca4
:ROAM_REFS: cite:tiongPlugandPlayVQAZeroshot2023
:END:
#+title: Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training

* Addressed Domains
Plug-and-play zero-shot Visual Question Answering (VQA) addresses vision and
language domains (i.e. question answering and image captioning tasks).

* Architectural Multimodality
As the model's name, it has a modular architecture that is built upon other
pretrained models and is connected by leveraging natural language and the
network interpretation. The architecture consists of three modules, that is,
1. image-question matching module to generate question-guided captions and
   network interpretation of the input image as shown by sampled \(K\) image
   patches that are calculated by leveraging the BLIP model and a variation of
   GradCAM technique,
2. image captioning module that generates \(N\) captions by stochastic top-\(k\)
   sampling, this module also leveraging the BLIP model,
3. question answering module using UnifiedQAv2 to answer the asked question
   given a concatenated encoded representations of individual caption from the
   image captioning module.

* Learning Paradigm
There is no learning paradigm in the proposed method. Instead, the proposed
method leverages pretrained models to solve the VQA problem.

* Key Innovations
The proposed method has a modular architecture, meaning it is flexible to
replace (e.g., plug-and-play) pretrained models inside the modules for further
performance improvements. Furthermore, the proposed network interpretation to
act as the interface between the visual language models (VLMs) and pretrained
language models (PLMs) improves the image caption generations which cover
relevant information to the question, thus improving question-answer accuracy.
Finally, the proposed method does not require additional training, thus the name
zero-shot VQA.

* Limitations
Since the proposed method relies on other pretrained models as the building
block, it inherits the bias given by the pretrained models. Furthermore, the
proposed method introduces additional inference cost due to its multi-step
process.
